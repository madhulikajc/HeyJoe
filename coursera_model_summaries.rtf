{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red241\green241\blue241;
\red45\green45\blue45;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;\cssrgb\c95568\c95568\c95568;
\cssrgb\c22990\c22990\c22990;}
\margl1440\margr1440\vieww15620\viewh11040\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \cb3 \CocoaLigature0 Coursera model\'92s GRU layers:\
\
\cf4 \cb5 \{'class_name': 'GRU', 'config': \{'name': 'gru_5', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': \{'class_name': 'VarianceScaling', 'config': \{'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None\}\}, 'recurrent_initializer': \{'class_name': 'Orthogonal', 'config': \{'gain': 1.0, 'seed': None\}\}, 'bias_initializer': \{'class_name': 'Zeros', 'config': \{\}\}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1, 'reset_after': False\}, 'name': 'gru_5', 'inbound_nodes': [[['dropout_7', 0, 0, \{\}]]]\},\cf2 \cb3 \
\
\
\
\cf4 \cb5 \{'class_name': 'GRU', 'config': \{'name': 'gru_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': \{'class_name': 'VarianceScaling', 'config': \{'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None\}\}, 'recurrent_initializer': \{'class_name': 'Orthogonal', 'config': \{'gain': 1.0, 'seed': None\}\}, 'bias_initializer': \{'class_name': 'Zeros', 'config': \{\}\}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 1, 'reset_after': False\}, 'name': 'gru_6', 'inbound_nodes': [[['batch_normalization_8', 0, 0, \{\}]]]\}\cf2 \cb3 \
\
\
\
My model:\
GRU layers\
\
\cf4 \cb5 \{'class_name': 'GRU', 'config': \{'name': 'gru', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': False, 'kernel_initializer': \{'class_name': 'GlorotUniform', 'config': \{'seed': None\}\}, 'recurrent_initializer': \{'class_name': 'Orthogonal', 'config': \{'gain': 1.0, 'seed': None\}\}, 'bias_initializer': \{'class_name': 'Zeros', 'config': \{\}\}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 2, 'reset_after': True\}, 'name': 'gru', 'inbound_nodes': [[['dropout', 0, 0, \{\}]]]\},\cf2 \cb3 \
\
\
\cf4 \cb5 \{'class_name': 'GRU', 'config': \{'name': 'gru_1', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': \{'class_name': 'GlorotUniform', 'config': \{'seed': None\}\}, 'recurrent_initializer': \{'class_name': 'Orthogonal', 'config': \{'gain': 1.0, 'seed': None\}\}, 'bias_initializer': \{'class_name': 'Zeros', 'config': \{\}\}, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0, 'implementation': 2, 'reset_after': True\}, 'name': 'gru_1', 'inbound_nodes': [[['batch_normalization_1', 0, 0, \{\}]]]\}\cf2 \cb3 \
\
\
Differences in the first GRU layer:\
\
with use_bias = True\
148224\
99072\
\
Differences in the second GRU layer:\
\
1. recurrent activation hard sigmoid in the Coursera and sigmoid in mine (Terminology changed? from versions)\
2. use_bias = True (so yes, Use bias is true, and we need to find a way to correct back the # params)\
3. kernel initializer is Glorot Uniform in mine and Variance Scaling in Coursera\'92s\
4. implementation: 2 instead of 1, (looks like default values may have changed)\
5. reset_after True in mine and False in the Coursera one (looks like default values are changed in different Python versions)\
\
Also note: We can see where the dropout rates are to be edited - would be very easy if there was a UI to do this, \
to take the Coursera model, change that parameter, and then train on my constructed data. However, choose to just rewrite my model to match the Coursera one, so I can use transfer learning and initialize with their weights. \
\
\
}