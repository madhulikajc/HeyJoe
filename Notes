
td_utils.py:

Code written by Coursera staff to be used in the programming assignment for 
Trigger Word Detection. Contains only a few functions which I use in Hey Joe -
contains graph_spectrogram to get the spectrogram x for a .wav file

And load_raw_audio() which loads all the audio files as Pydub Audiosegments and
ready to be turned into spectrograms which are then used as the input to the
machine learning algorithm



heyjoe_helper_functions.py:

Code that I wrote for the Coursera Problem set / Programming Assignment for 
Trigger Word Detection. Contains the code to create synthetic training examples from
activates, negatives and backgrounds



heyjoe_create_examples.py:
Code I wrote to create HeyJoe activate word containing examples, examples also
contain negatives. After 700-1400 examples have been created, they are saved to disk
in X700.npy and a corresponding Y700.npy. This code  uses the helper functions above.



heyjoe_model<n>.py
Code I wrote to create the Speech Recognition sequence model using GRU layers in Keras, as
well as Dropout regularization and conv 1D layer. Load weights from the Coursera model I 
wrote in the problem set, which Coursera trains on a GPU. These weights are from a network
with same architecture, but trained on the activate word "Activate" (not "Hey Joe").
Use transfer learning, to initialize my model with these weights, and then learn for
several epochs on examples I created above, where the activate word is "Hey Joe".
Save model to disk at the end.

Use also several real world examples of Hey Joe, and instructions to Spotify, 
and hand label those. This dev set (created in other python files, see below) is 
partially used to test different models, and also partially mixed into the training
set (a different subset from the ones used in testing). 


heyjoe.py
Eventually write code to listen through the microphone and check intervals for Hey Joe.
If we hear the trigger word, then we should use Google Cloud Speech Recognition to 
recognize what is said immediately afterwards, and then send that output to Spotify 
Note: after running the models and examining/spot checking several real world examples, it would make sense to send a few seconds worth of the Speech recording BEFORE the trigger
word detection labels to Google Cloud Speech Recognition too (as the model I trained
can be a bit delayed in the labels, especially if Hey Joe Play is uttered in one
single string by the user. 



Notes on the Saved models:

# originally I had saved a model by having loaded the Coursera model and run it directly on new examples with new activate word "Hey Joe", but the dropout rate is too high (0.8) as the argument to  Dropout layer in Keras changed from keep_prob to dropout rate (from the Coursera version to the version of Python/Tensorflow/Keras I installed on my computer.
So this was not ideal at all and did not work. So I deleted this model,
and recreated the whole network architecture and only loaded the weights from my Coursera
problem set model, to have transfer learning. 


my_model1 was trained using: 
1) a new model which I created to match the architecture used in the one that
Coursera trained on a GPU for a few days, but the Coursera one was trained on a different trigger word. 
2) Weights from the Coursera model trained on a different trigger/activate word (tr_model.h5)
3) Several epochs of further training using my examples which were labeled according to the new trigger word (Hey Joe). These examples are loaded from X700.npy and Y700.npy

my_model2 was trained for another 10 epochs

my_model3 was trained using some new backgrounds from real world scenarios

my_model4 was trained using some examples which contained 44.1 kHz activates and backgrounds, and surprisingly? this resulted in poor initial performance by my_model3 which was trained on 16 kHz examples primarily. 


2/22/2020 evening is when I switched all recordings over to 44.1kHz (9:30 pm). Previous wav files were at 16 kHz if they were from my phone, and 44.1 if they were downloaded
from the internet.


2/24/2020:
Noticed a huge bug, which is I had forgotten to remove the random seed fixed, in order for testing (and originally for the Coursera grader) in one of the functions I used from
my programming assignment. This means the training examples are not random, and real 
learning was not happening. Delete models and start over. Remake all training sets. 


*Also number of backgrounds increased (I recorded 3 more) so now it's 10 backgrounds, and 1000 examples if I do 100 per background


2/x/2020
Added 4 more backgrounds for a total of 14 backgrounds (these backgrounds are from real world recordings of us talking and so on). The remaining of these recordings will be used for a dev set. 

Total of 14 backgrounds, and therefore recent training data is 1400 examples per set


2/29/2020-3/1/2020

my_model7.h5 is the result of training on X1400a-e.npy
A good starting point for new training of models. 

my_model8.h5 is the result of training on X2800 (which I constructed similar to X1400a-e, but I don't quite remember the parameters) and it was interrupted and the 
Python interpreter took up again in the 20th epoch. 

NEXT STEP: Add to the training set a real world, hand labeled set of dev set type examples. Use my spectrogram printing code (disp_wav.py), and zoom in using ylim. This way, I hand labeled 25 files. 

Use a part of these in training, use the rest in development and testing.

By themselves: and starting with my_model8.h5 I created my_model9_hand_label25.h5

I noticed this performed less well on the train.wav (created example), and still no good on the dev set real world Hey Joe recordings. Not enough data and training cycles. 

Try again, start with my_model9_hand_label25, use a data set X1400a + 25 hand labeled data sets, and train again with batch size of 10, and 10 epochs, and see how we do.

my_model10_1400a_plushandlabel25.h5

Now take my_model10... and add in 1400b, but the same 25 training examples hand labeled real world ones

performance maybe slightly improved and 0.11 instead of 0.0003 in the range of the supposed trigger word.

Used 10 epochs until 1400d. 

Then trained for 30 epochs: my_model13 was trained on 30 epochs
any better? YES! The positions that should be 1, are now 0.22, and the rest are 0.02 on the test/dev examples.

my_model14 was also 30 epochs

my_model15 was 30 epochs on 1400a

my_model14 and my_model16 were both the best with my_model17 having less good
(spot checking on hand labeled examples which were not used in training) - use a dev set and choose the best one after a few more training sessions


---------
AWS Amazon Sagemaker

GPU: Sagemaker, finally working, see models trained on there. Not much faster when 
mini batch size = 10, but once batch size = 200 or 100, much much faster than my computer, pretty cool to see the GPU optimization for large batch sizes


Model names nomenclature: the number tells the lineage - my_model17 was trained using my_model_16 as a starting point.  We have a few different 17s, so the 18s were trained on different 17s, but I tried to make notes on #epochs, mini batch size, and training set used when naming the model. 

When _sm at the end of the Model Name, then it means the model was trained using an Amazon Sagemaker instance 
---

Noticed three things, model performance on 11.wav is good at my_model16, but degraded after running on more examples. 

Two theories:
1) background correction is making the synthetic training examples not as good as the real world ones? Or maybe activates need to be made softer? 

2) I noticed that in practice many examples have zero activates. This is because I decreased the range to 0..2 (ie 0 or 1 activates) because otherwise it was not fitting in the background along with the negatives. This means that 50% of the samples don't have an example of Hey Joe. This needs to be fixed. So I changed the range to 0..3- ie 0-2 activates, but then it hangs often, so I put in a line where if the number_of_activates is chosen to be 2, then the number of negatives is either 0 or 1, and this seems to have worked to generate a synthetic data set of 1400. 

This training set (with higher number of activates but keeping the original background correction) is X14002a.npy, Y14002a.npy



Also, investigate background reduction and things like that for the training examples.
what's the real problem here with the created training examples?

Tried this, and:

Non bg correction seems to be working worse (1400_2b_no_bg_corr.npy)

The increase of Hey Joe in the training examples seems to have alleviated the problem significantly - 14002a model17 seems the best by spot checking. Bg correction does not seem to the issue.


----
To do:

Check comprehensively on a dev set using precision and recall. 

Shuffle and save a shuffled set?

INVESTIGATE and TEST other METRICS and optimization parameters


*****

Cost of Sagemaker ml.p2.xlarge
storage: $0.10/gig/month = $5 / month for 50 gigs
compute: $1.26/hour if notebook instance is in service, otherwise Stop







 